{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df766bbd",
   "metadata": {},
   "source": [
    "# Configure packages and filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082bcc80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T21:45:34.533734Z",
     "start_time": "2023-08-24T21:45:34.501109Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd67ec73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T21:45:36.961035Z",
     "start_time": "2023-08-24T21:45:34.787584Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import math\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from pyproj import CRS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b083a3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T21:45:36.988966Z",
     "start_time": "2023-08-24T21:45:36.964545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "# Which is one directory above notebooks/\n",
    "ABS_DIR = os.path.abspath(Path(os.getcwd()).parents[0])\n",
    "# Get raw data directory\n",
    "FR = join(ABS_DIR, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "FI = join(ABS_DIR, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "FP = join(ABS_DIR, 'data', 'processed')\n",
    "\n",
    "# Directories for exposure, vulnerability (vuln) and \n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, 'exposure')\n",
    "VULN_DIR_R = join(FR, 'vuln')\n",
    "REF_DIR_R = join(FR, 'ref')\n",
    "# Haz is for FEMA NFHL and depth grids\n",
    "HAZ_DIR_R = join(FR, 'haz')\n",
    "\n",
    "# Make sure directories exist\n",
    "Path(EXP_DIR_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(VULN_DIR_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(REF_DIR_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(HAZ_DIR_R).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e0ad8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T21:45:40.808094Z",
     "start_time": "2023-08-24T21:45:40.768544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants (could be replaced by config files or user input)\n",
    "# County fips (list for some scalability)\n",
    "FIPS = ['42101']\n",
    "\n",
    "# FEMA \"chunk\" size for API\n",
    "CHUNK_FEMA = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544bbd1a",
   "metadata": {},
   "source": [
    "# Exposure Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf99e1",
   "metadata": {},
   "source": [
    "## National Structure Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bb9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL\n",
    "url = \"https://nsi.sec.usace.army.mil/nsiapi/structures\"\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the NSI API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for NSI DFs\n",
    "nsi_df_list = []\n",
    "\n",
    "for fips in FIPS:\n",
    "    # GET Request\n",
    "    nsi_get = requests.get(url + '?fips=' + fips)\n",
    "    \n",
    "    # Temp data frame\n",
    "    temp = pd.json_normalize(nsi_get.json()['features'])\n",
    "    \n",
    "    # Add to list\n",
    "    nsi_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nsi = pd.concat(nsi_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nsi.to_parquet(join(EXP_DIR_R, 'nsi.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d8a50",
   "metadata": {},
   "source": [
    "## Local Municipal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ba1d4",
   "metadata": {},
   "source": [
    "### Parcels/Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb5e8c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T19:44:39.490280Z",
     "start_time": "2023-07-26T19:42:32.896157Z"
    }
   },
   "outputs": [],
   "source": [
    "# PWD parcels (Need BRT_ID and PIN for merge with OPA data)\n",
    "# These will be used for linking assessment data from OPA\n",
    "# with building footprints based on attribute merges\n",
    "\n",
    "# Metadata link\n",
    "# https://metadata.phila.gov/#home/datasetdetails/\n",
    "# 5543864620583086178c4e7a/representationdetails/55438a829b989a05172d0cfa/\n",
    "\n",
    "# Download the geojson\n",
    "# https://opendata.arcgis.com/datasets/\n",
    "# 84baed491de44f539889f2af178ad85c_0.geojson\n",
    "\n",
    "url = (\"https://opendata.arcgis.com/datasets/\"\n",
    "       + \"84baed491de44f539889f2af178ad85c_0.geojson\")\n",
    "\n",
    "# GET request for pwd parcel data\n",
    "pwd_get = requests.get(url)\n",
    "\n",
    "# Json object to access data and crs\n",
    "get_json = pwd_get.json()\n",
    "\n",
    "# Get coordinate reference system\n",
    "crs = get_json['crs']['properties']['name']\n",
    "# Get geographic crs\n",
    "crs_geo = CRS.from_user_input(crs)\n",
    "\n",
    "# Get features of request\n",
    "temp = get_json['features']\n",
    "\n",
    "# Normalize into dataframe\n",
    "temp_df = pd.json_normalize(temp)\n",
    "\n",
    "# Get the geometry from each row\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Get geodataframe from last two steps\n",
    "pwd_geo = gpd.GeoDataFrame(temp_df,\n",
    "                           crs=crs_geo,\n",
    "                           geometry=temp_geo)\n",
    "\n",
    "# Need to drop geometry.coordinates to write file\n",
    "# Also can drop type, geometry.type\n",
    "drop_col = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "pwd_geo_out = pwd_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write file\n",
    "pwd_geo_out.to_file(join(EXP_DIR_R, 'pc_pwd.gpkg'),\n",
    "                    driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08d2579e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T20:17:22.176611Z",
     "start_time": "2023-07-26T20:16:54.364284Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_770026/881545838.py:18: DtypeWarning: Columns (4,12,21,25,30,54,67,69,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  opa = pd.read_csv(url)\n"
     ]
    }
   ],
   "source": [
    "# OPA Parcels (These have all the assessments)\n",
    "# only point based data\n",
    "\n",
    "# Metadata link\n",
    "# https://metadata.phila.gov/#home/datasetdetails/\n",
    "# 5543865f20583086178c4ee5/representationdetails/\n",
    "# 55d624fdad35c7e854cb21a4/?view_287_per_page=100&view_287_page=1\n",
    "\n",
    "# API endpoint, .csv\n",
    "# https://opendata-downloads.s3.amazonaws.com/opa_properties_public.csv\n",
    "# Since this is just a point database and we're linking it to\n",
    "# building footprints through pwd, I think downloading\n",
    "# csv is simpler and more straight forward\n",
    "url = (\"https://opendata-downloads.s3.amazonaws.com/\"\n",
    "       + \"opa_properties_public.csv\")\n",
    "\n",
    "# Pandas can read directly from the url\n",
    "opa = pd.read_csv(url)\n",
    "\n",
    "# Write out file as csv\n",
    "opa_out.to_csv(join(EXP_DIR_R, 'pc_opa.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ea5a2",
   "metadata": {},
   "source": [
    "### Building Footprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5faa437b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T20:22:38.629122Z",
     "start_time": "2023-07-26T20:20:31.097184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Building footprint polygons with PARCEL_ID_ \n",
    "# which links to bld parcels\n",
    "\n",
    "# Metadata link\n",
    "# https://metadata.phila.gov/#home/datasetdetails/\n",
    "# 5543864f20583086178c4ea5/representationdetails/595e8e85ac27025c82c53c7c/\n",
    "\n",
    "# API endpoint, .geojson\n",
    "# https://opendata.arcgis.com/datasets/\n",
    "# ab9e89e1273f445bb265846c90b38a96_0.geojson\n",
    "\n",
    "# This is geojson like bld above so code can be same\n",
    "# Makes sense to have a function that does this stuff, but\n",
    "# for a single county case-study like this for the framework\n",
    "# it could be more interpretable to maintain more readability\n",
    "url = (\"https://opendata.arcgis.com/datasets/\"\n",
    "       + \"ab9e89e1273f445bb265846c90b38a96_0.geojson\")\n",
    "\n",
    "# GET request for bld footprint data\n",
    "bld_get = requests.get(url)\n",
    "\n",
    "# Json object to access data and crs\n",
    "get_json = bld_get.json()\n",
    "\n",
    "# Get coordinate reference system\n",
    "crs = get_json['crs']['properties']['name']\n",
    "# Get geographic crs\n",
    "crs_geo = CRS.from_user_input(crs)\n",
    "\n",
    "# Get features of request\n",
    "temp = get_json['features']\n",
    "\n",
    "# Normalize into dataframe\n",
    "temp_df = pd.json_normalize(temp)\n",
    "\n",
    "# Get the geometry from each row\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Get geodataframe from last two steps\n",
    "bld_geo = gpd.GeoDataFrame(temp_df,\n",
    "                           crs=crs_geo,\n",
    "                           geometry=temp_geo)\n",
    "\n",
    "# Need to drop geometry.coordinates to write file\n",
    "# Also can drop type, geometry.type\n",
    "drop_col = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "bld_geo_out = bld_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write file\n",
    "bld_geo_out.to_file(join(EXP_DIR_R, 'bld_fp.gpkg'),\n",
    "                    driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391063c3",
   "metadata": {},
   "source": [
    "## FEMA Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128de764",
   "metadata": {},
   "source": [
    "### NFIP Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b0d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL for querying policies\n",
    "url = \"https://www.fema.gov/api/open/v2/FimaNfipPolicies?$\"\n",
    "# Get the URL for # policies that meet request\n",
    "check = url + \"inlinecount=allpages&$top=1&$select=id&$\"\n",
    "\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the Pols API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for Pols DFs\n",
    "pol_df_list = []\n",
    "\n",
    "# NFIP API usage adapts R code here: https://docs.ropensci.org/rfema/\n",
    "# And follows OpenFEMA guide: \n",
    "# https://www.fema.gov/about/openfema/working-with-large-data-sets#app-a\n",
    "\n",
    "for fips in FIPS:\n",
    "    # County endpoint\n",
    "    c_end = \"filter=countyCode%20eq%20%27\" + fips + \"%27\"\n",
    "    \n",
    "    # First, get the total number of records\n",
    "    records = requests.get(check + c_end)\n",
    "    n_rec = pd.json_normalize(records.json())['metadata.count'][0]\n",
    "    \n",
    "    # Get iterations needed (1,000 record limit)\n",
    "    iterations = math.ceil(n_rec / CHUNK_FEMA)\n",
    "    \n",
    "    # Now, download 1,000 records at a time and store in list\n",
    "    # Loop through required iterations and keep appending policy \n",
    "    # data from the GET request to the pol_df_list\n",
    "    for i in range(iterations):\n",
    "        skip_str = \"&$skip=\" + str(i*CHUNK_FEMA)\n",
    "    \n",
    "        # GET Request\n",
    "        pol_get = requests.get(url + c_end + skip_str)\n",
    "\n",
    "        # Temp data frame\n",
    "        temp = pd.json_normalize(pol_get.json()['FimaNfipPolicies'])\n",
    "\n",
    "        # Add to list\n",
    "        pol_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nfip_pol = pd.concat(pol_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nfip_pol.to_parquet(join(EXP_DIR_R, 'nfip_pols.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72e601",
   "metadata": {},
   "source": [
    "### NFIP Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c38810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL for querying claimicies\n",
    "url = \"https://www.fema.gov/api/open/v2/FimaNfipClaims?$\"\n",
    "# Get the URL for # claimicies that meet request\n",
    "check = url + \"inlinecount=allpages&$top=1&$select=id&$\"\n",
    "\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the claims API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for claims DFs\n",
    "claim_df_list = []\n",
    "\n",
    "# NFIP API usage adapts R code here: https://docs.ropensci.org/rfema/\n",
    "# And follows OpenFEMA guide: \n",
    "# https://www.fema.gov/about/openfema/working-with-large-data-sets#app-a\n",
    "\n",
    "for fips in FIPS:\n",
    "    # County endpoint\n",
    "    c_end = \"filter=countyCode%20eq%20%27\" + fips + \"%27\"\n",
    "    \n",
    "    # First, get the total number of records\n",
    "    records = requests.get(check + c_end)\n",
    "    n_rec = pd.json_normalize(records.json())['metadata.count'][0]\n",
    "    \n",
    "    # Get iterations needed (1,000 record limit)\n",
    "    iterations = math.ceil(n_rec / CHUNK_FEMA)\n",
    "    \n",
    "    # Now, download 1,000 records at a time and store in list\n",
    "    # Loop through required iterations and keep appending claimicy \n",
    "    # data from the GET request to the claim_df_list\n",
    "    for i in range(iterations):\n",
    "        skip_str = \"&$skip=\" + str(i*CHUNK_FEMA)\n",
    "    \n",
    "        # GET Request\n",
    "        claim_get = requests.get(url + c_end + skip_str)\n",
    "\n",
    "        # Temp data frame\n",
    "        temp = pd.json_normalize(claim_get.json()['FimaNfipClaims'])\n",
    "\n",
    "        # Add to list\n",
    "        claim_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nfip_claim = pd.concat(claim_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nfip_claim.to_parquet(join(EXP_DIR_R, 'nfip_claims.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78797fb8",
   "metadata": {},
   "source": [
    "## Federal Housing Finance Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf17202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T21:51:17.690928Z",
     "start_time": "2023-08-24T21:51:16.891813Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a script of helpful functions and add this\n",
    "# Helper function for downloading zip files\n",
    "# from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d1a93b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T21:52:47.239118Z",
     "start_time": "2023-08-24T21:52:45.823221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Want to download housing price index for deflating\n",
    "# market values to a standard \n",
    "\n",
    "# https://www.fhfa.gov/DataTools/Downloads/Pages/\n",
    "# House-Price-Index-Datasets.aspx#qat\n",
    "\n",
    "# Use annual house price indices\n",
    "# Counties (Developmental Index; Not Seasonally Adjusted)\n",
    "# https://www.fhfa.gov/DataTools/Downloads/Documents/HPI/\n",
    "# HPI_AT_BDL_county.xlsx\n",
    "url = (\"https://www.fhfa.gov/DataTools/Downloads/Documents/HPI/\"\n",
    "       + \"HPI_AT_BDL_county.xlsx\")\n",
    "\n",
    "# Destination path\n",
    "dst_path = join(EXP_DIR_R, 'hpi_county.xlsx')\n",
    "\n",
    "# Download data\n",
    "download_url(url, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14448f4",
   "metadata": {},
   "source": [
    "# Hazard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80b44ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T21:20:38.280086Z",
     "start_time": "2023-08-22T21:20:37.817507Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a script of helpful functions and add this\n",
    "# Helper function for downloading zip files\n",
    "# from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565d83a",
   "metadata": {},
   "source": [
    "## NFHL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a49493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T18:05:11.210527Z",
     "start_time": "2023-08-22T18:05:02.853935Z"
    }
   },
   "outputs": [],
   "source": [
    "# I went to FEMA Flood Map Service Center\n",
    "# I chose Philadelphia County from the drop down menus\n",
    "# I got the following link for the current county NFHL after\n",
    "# downloading & cancelling the download\n",
    "# https://map1.msc.fema.gov/data/FRP/FRD_02040202_PA_GeoTIFFs_20160801\n",
    "# .zip?LOC=ccad78e48360e7a0a5cf6848dfa4db11\n",
    "\n",
    "# I went to FEMA Flood Map Service Center\n",
    "# I chose Philadelphia County from the drop down menus\n",
    "# I got the following link for GeoTIFFs for the Flood Risk Database\n",
    "# https://hazards.fema.gov/nfhlv2/output/County/420757_20230701.zip\n",
    "url = (\"https://hazards.fema.gov/nfhlv2/output/County/420757_20230701.zip\")\n",
    "\n",
    "# Destination file directory\n",
    "dst = Path(join(HAZ_DIR_R, 'nfhl'))\n",
    "dst.mkdir(parents=True, exist_ok=True)\n",
    "# Destination path\n",
    "dst_path = join(dst, 'nfhl.zip')\n",
    "\n",
    "# Download nfhl\n",
    "download_url(url, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c42a5",
   "metadata": {},
   "source": [
    "## Depth Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5385326e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T21:29:43.144368Z",
     "start_time": "2023-08-22T21:21:20.429527Z"
    }
   },
   "outputs": [],
   "source": [
    "# I went to FEMA Flood Map Service Center\n",
    "# I chose Philadelphia County from the drop down menus\n",
    "# I got the following link for GeoTIFFs for the Flood Risk Database\n",
    "# \"https://map1.msc.fema.gov/data/FRP/FRD_02040202_PA_GeoTIFFs_20160801\" +\n",
    "# \".zip?LOC=ccad78e48360e7a0a5cf6848dfa4db11\"\n",
    "# This takes a while to download because it's a large file\n",
    "# You can confirm the endpoint for this download by following the steps, \n",
    "# clicking download on the DL icon on the webpage, immediately\n",
    "# cancelling the download, and checking your browser's download\n",
    "# page to see what server the download happens from\n",
    "# I did these steps on Google Chrome 114.0.5735.133\n",
    "\n",
    "url = (\"https://map1.msc.fema.gov/data/FRP/FRD_02040202_PA_GeoTIFFs_20160801\"\n",
    "       + \".zip?LOC=ccad78e48360e7a0a5cf6848dfa4db11\")\n",
    "\n",
    "# Destination file directory\n",
    "dst = Path(join(HAZ_DIR_R, 'dg'))\n",
    "dst.mkdir(parents=True, exist_ok=True)\n",
    "# Destination path\n",
    "dst_path = join(dst, 'dg.zip')\n",
    "\n",
    "# Download depth grids\n",
    "download_url(url, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40910ef",
   "metadata": {},
   "source": [
    "# Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ec292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a script of helpful functions and add this\n",
    "# Helper function for downloading zip files\n",
    "# from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18d2ea",
   "metadata": {},
   "source": [
    "## Social Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a7db06a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T19:19:13.200212Z",
     "start_time": "2023-08-22T19:19:09.913355Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOAA SOVI\n",
    "url = 'https://coast.noaa.gov/htdata/SocioEconomic/SoVI2010/SoVI_2010_PA.zip'\n",
    "save_path = join(VULN_DIR_R, 'social', 'noaa.zip')\n",
    "# Make sure parent directory exists\n",
    "# TODO: There could be a useful helper function for this\n",
    "Path(save_path).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Request and write\n",
    "download_url(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d68bb27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T19:54:32.194041Z",
     "start_time": "2023-08-22T19:54:27.478667Z"
    }
   },
   "outputs": [],
   "source": [
    "# CEJST\n",
    "# Data from https://screeningtool.geoplatform.gov/en/downloads\n",
    "url = ('https://static-data-screeningtool.geoplatform.gov/data-versions/'\n",
    "       + '1.0/data/score/downloadable/1.0-communities.csv')\n",
    "\n",
    "save_path = join(VULN_DIR_R, 'social', 'cejst.csv')\n",
    "\n",
    "# Make sure parent directory exists\n",
    "# TODO: There could be a useful helper function for this\n",
    "Path(save_path).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Request and write\n",
    "download_url(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dab45c92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T19:32:54.733702Z",
     "start_time": "2023-08-22T19:32:54.065833Z"
    }
   },
   "outputs": [],
   "source": [
    "# FHA LMI\n",
    "# Data from https://www.hudexchange.info/programs/acs-low-mod-summary-data/\n",
    "# acs-low-mod-summary-data-block-groups-places/\n",
    "\n",
    "url = ('https://www.hudexchange.info/sites/onecpd/assets/File/'\n",
    "       + 'ACS_2015_lowmod_blockgroup_all.xlsx')\n",
    "\n",
    "# Unfortunately xlsx file\n",
    "# But you can use openpyxl engine with pd.read_excel\n",
    "save_path = join(VULN_DIR_R, 'social', 'lmi.xlsx')\n",
    "\n",
    "# Make sure parent directory exists\n",
    "# TODO: There could be a useful helper function for this\n",
    "Path(save_path).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Request and write\n",
    "download_url(url, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3d145",
   "metadata": {},
   "source": [
    "# Administrative Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f217aab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T20:10:55.753593Z",
     "start_time": "2023-08-22T20:10:55.302101Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: It would be much better to have helper \n",
    "# functions for this in the future, especially if\n",
    "# using generic reference data (like from TIGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1951e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T20:10:04.910022Z",
     "start_time": "2023-08-22T20:10:04.185043Z"
    }
   },
   "outputs": [],
   "source": [
    "# County boundary (city limits)\n",
    "# Metadata link\n",
    "# https://metadata.phila.gov/#home/datasetdetails/\n",
    "# 5543868820583086178c4f89/representationdetails/55438ada9b989a05172d0d92/\n",
    "\n",
    "# Download the geojson\n",
    "# https://opendata.arcgis.com/datasets/\n",
    "# 405ec3da942d4e20869d4e1449a2be48_0.geojson\n",
    "\n",
    "url = (\"https://opendata.arcgis.com/datasets/\"\n",
    "       + \"405ec3da942d4e20869d4e1449a2be48_0.geojson\")\n",
    "\n",
    "# GET request for city limits\n",
    "city_get = requests.get(url)\n",
    "\n",
    "# Json object to access data and crs\n",
    "get_json = city_get.json()\n",
    "\n",
    "# Get coordinate reference system\n",
    "crs = get_json['crs']['properties']['name']\n",
    "# Get geographic crs\n",
    "crs_geo = CRS.from_user_input(crs)\n",
    "\n",
    "# Get features of request\n",
    "temp = get_json['features']\n",
    "\n",
    "# Normalize into dataframe\n",
    "temp_df = pd.json_normalize(temp)\n",
    "\n",
    "# Get the geometry from each row\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Get geodataframe from last two steps\n",
    "city_geo = gpd.GeoDataFrame(temp_df,\n",
    "                            crs=crs_geo,\n",
    "                            geometry=temp_geo)\n",
    "\n",
    "# Need to drop geometry.coordinates to write file\n",
    "# Also can drop type, geometry.type\n",
    "drop_col = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "city_geo_out = city_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write file\n",
    "city_geo_out.to_file(join(REF_DIR_R, 'city.gpkg'),\n",
    "                     driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62ef603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T20:12:03.530079Z",
     "start_time": "2023-08-22T20:12:01.685212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Census tracts, 2010\n",
    "# Metadata link\n",
    "# https://metadata.phila.gov/#home/datasetdetails/5543867720583086178c4f47/\n",
    "# representationdetails/55438aca9b989a05172d0d7a/\n",
    "\n",
    "# Download the geojson\n",
    "# https://opendata.arcgis.com/datasets/\n",
    "# 8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\n",
    "\n",
    "url = (\"https://opendata.arcgis.com/datasets/\"\n",
    "       + \"8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\")\n",
    "\n",
    "# GET request for tracts\n",
    "tract_get = requests.get(url)\n",
    "\n",
    "# Json object to access data and crs\n",
    "get_json = tract_get.json()\n",
    "\n",
    "# Get coordinate reference system\n",
    "crs = get_json['crs']['properties']['name']\n",
    "# Get geographic crs\n",
    "crs_geo = CRS.from_user_input(crs)\n",
    "\n",
    "# Get features of request\n",
    "temp = get_json['features']\n",
    "\n",
    "# Normalize into dataframe\n",
    "temp_df = pd.json_normalize(temp)\n",
    "\n",
    "# Get the geometry from each row\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Get geodataframe from last two steps\n",
    "tract_geo = gpd.GeoDataFrame(temp_df,\n",
    "                             crs=crs_geo,\n",
    "                             geometry=temp_geo)\n",
    "\n",
    "# Need to drop geometry.coordinates to write file\n",
    "# Also can drop type, geometry.type\n",
    "drop_col = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "tract_geo_out = tract_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write file\n",
    "tract_geo_out.to_file(join(REF_DIR_R, 'tracts.gpkg'),\n",
    "                      driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8c93e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T20:13:06.784295Z",
     "start_time": "2023-08-22T20:13:04.346150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Census block groups, 2010\n",
    "# Metadata link\n",
    "# https://metadata.phila.gov/#home/datasetdetails/\n",
    "# 5543867720583086178c4f46/representationdetails/55438ac99b989a05172d0d79/\n",
    "\n",
    "# Download the geojson\n",
    "# https://opendata.arcgis.com/datasets/\n",
    "# 8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\n",
    "\n",
    "url = (\"https://opendata.arcgis.com/datasets/\"\n",
    "       + \"2f982bada233478ea0100528227febce_0.geojson\")\n",
    "\n",
    "# GET request for blocks\n",
    "block_get = requests.get(url)\n",
    "\n",
    "# Json object to access data and crs\n",
    "get_json = block_get.json()\n",
    "\n",
    "# Get coordinate reference system\n",
    "crs = get_json['crs']['properties']['name']\n",
    "# Get geographic crs\n",
    "crs_geo = CRS.from_user_input(crs)\n",
    "\n",
    "# Get features of request\n",
    "temp = get_json['features']\n",
    "\n",
    "# Normalize into dataframe\n",
    "temp_df = pd.json_normalize(temp)\n",
    "\n",
    "# Get the geometry from each row\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Get geodataframe from last two steps\n",
    "block_geo = gpd.GeoDataFrame(temp_df,\n",
    "                             crs=crs_geo,\n",
    "                             geometry=temp_geo)\n",
    "\n",
    "# Need to drop geometry.coordinates to write file\n",
    "# Also can drop type, geometry.type\n",
    "drop_col = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "block_geo_out = block_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write file\n",
    "block_geo_out.to_file(join(REF_DIR_R, 'blocks.gpkg'),\n",
    "                      driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790f5629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T20:15:47.575917Z",
     "start_time": "2023-08-22T20:15:46.392586Z"
    }
   },
   "outputs": [],
   "source": [
    "# Zip codes (Important: zip codes, not ZCTA)\n",
    "\n",
    "# Metadata link\n",
    "# https://metadata.phila.gov/#home/datasetdetails/\n",
    "# 555f813af15fcb6c6ed44153/representationdetails/5589aa52b80410802d7e643b/\n",
    "\n",
    "# Download the geojson\n",
    "# https://opendata.arcgis.com/datasets/\n",
    "# b54ec5210cee41c3a884c9086f7af1be_0.geojson\n",
    "\n",
    "url = (\"https://opendata.arcgis.com/datasets/\"\n",
    "       + \"b54ec5210cee41c3a884c9086f7af1be_0.geojson\")\n",
    "\n",
    "# GET request for zipss\n",
    "zips_get = requests.get(url)\n",
    "\n",
    "# Json object to access data and crs\n",
    "get_json = zips_get.json()\n",
    "\n",
    "# Get coordinate reference system\n",
    "crs = get_json['crs']['properties']['name']\n",
    "# Get geographic crs\n",
    "crs_geo = CRS.from_user_input(crs)\n",
    "\n",
    "# Get features of request\n",
    "temp = get_json['features']\n",
    "\n",
    "# Normalize into dataframe\n",
    "temp_df = pd.json_normalize(temp)\n",
    "\n",
    "# Get the geometry from each row\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Get geodataframe from last two steps\n",
    "zips_geo = gpd.GeoDataFrame(temp_df,\n",
    "                            crs=crs_geo,\n",
    "                            geometry=temp_geo)\n",
    "\n",
    "# Need to drop geometry.coordinates to write file\n",
    "# Also can drop type, geometry.type\n",
    "drop_col = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "zips_geo_out = zips_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write file\n",
    "zips_geo_out.to_file(join(REF_DIR_R, 'zips.gpkg'),\n",
    "                     driver='GPKG')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icom_risk",
   "language": "python",
   "name": "icom_risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
