{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df766bbd",
   "metadata": {},
   "source": [
    "# Configure packages and filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082bcc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd67ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import math\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b083a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "# Which is one directory above notebooks/\n",
    "ABS_DIR = os.path.abspath(Path(os.getcwd()).parents[0])\n",
    "# Get raw data directory\n",
    "FR = join(ABS_DIR, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "FI = join(ABS_DIR, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "FP = join(ABS_DIR, 'data', 'processed')\n",
    "\n",
    "# Directories for exposure, vulnerability (vuln) and \n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, 'exposure')\n",
    "VULN_DIR_R = join(FR, 'vuln')\n",
    "REF_DIR_R = join(FR, 'ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e0ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (could be replaced by config files or user input)\n",
    "# County fips (list for some scalability)\n",
    "FIPS = ['42101']\n",
    "\n",
    "# FEMA \"chunk\" size for API\n",
    "CHUNK_FEMA = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544bbd1a",
   "metadata": {},
   "source": [
    "# Exposure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f775649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare file directories\n",
    "Path(EXP_DIR_R).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf99e1",
   "metadata": {},
   "source": [
    "## National Structure Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bb9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL\n",
    "url = \"https://nsi.sec.usace.army.mil/nsiapi/structures\"\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the NSI API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for NSI DFs\n",
    "nsi_df_list = []\n",
    "\n",
    "for fips in FIPS:\n",
    "    # GET Request\n",
    "    nsi_get = requests.get(url + '?fips=' + fips)\n",
    "    \n",
    "    # Temp data frame\n",
    "    temp = pd.json_normalize(nsi_get.json()['features'])\n",
    "    \n",
    "    # Add to list\n",
    "    nsi_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nsi = pd.concat(nsi_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nsi.to_parquet(join(EXP_DIR_R, 'nsi.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d8a50",
   "metadata": {},
   "source": [
    "## Local Municipal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391063c3",
   "metadata": {},
   "source": [
    "## NFIP Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128de764",
   "metadata": {},
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b0d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL for querying policies\n",
    "url = \"https://www.fema.gov/api/open/v2/FimaNfipPolicies?$\"\n",
    "# Get the URL for # policies that meet request\n",
    "check = url + \"inlinecount=allpages&$top=1&$select=id&$\"\n",
    "\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the Pols API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for Pols DFs\n",
    "pol_df_list = []\n",
    "\n",
    "# NFIP API usage adapts R code here: https://docs.ropensci.org/rfema/\n",
    "# And follows OpenFEMA guide: \n",
    "# https://www.fema.gov/about/openfema/working-with-large-data-sets#app-a\n",
    "\n",
    "for fips in FIPS:\n",
    "    # County endpoint\n",
    "    c_end = \"filter=countyCode%20eq%20%27\" + fips + \"%27\"\n",
    "    \n",
    "    # First, get the total number of records\n",
    "    records = requests.get(check + c_end)\n",
    "    n_rec = pd.json_normalize(records.json())['metadata.count'][0]\n",
    "    \n",
    "    # Get iterations needed (1,000 record limit)\n",
    "    iterations = math.ceil(n_rec / CHUNK_FEMA)\n",
    "    \n",
    "    # Now, download 1,000 records at a time and store in list\n",
    "    # Loop through required iterations and keep appending policy \n",
    "    # data from the GET request to the pol_df_list\n",
    "    for i in range(iterations):\n",
    "        skip_str = \"&$skip=\" + str(i*CHUNK_FEMA)\n",
    "    \n",
    "        # GET Request\n",
    "        pol_get = requests.get(url + c_end + skip_str)\n",
    "\n",
    "        # Temp data frame\n",
    "        temp = pd.json_normalize(pol_get.json()['FimaNfipPolicies'])\n",
    "\n",
    "        # Add to list\n",
    "        pol_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nfip_pol = pd.concat(pol_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nfip_pol.to_parquet(join(EXP_DIR_R, 'nfip_pols.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72e601",
   "metadata": {},
   "source": [
    "### Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c38810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL for querying claimicies\n",
    "url = \"https://www.fema.gov/api/open/v2/FimaNfipClaims?$\"\n",
    "# Get the URL for # claimicies that meet request\n",
    "check = url + \"inlinecount=allpages&$top=1&$select=id&$\"\n",
    "\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the claims API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for claims DFs\n",
    "claim_df_list = []\n",
    "\n",
    "# NFIP API usage adapts R code here: https://docs.ropensci.org/rfema/\n",
    "# And follows OpenFEMA guide: \n",
    "# https://www.fema.gov/about/openfema/working-with-large-data-sets#app-a\n",
    "\n",
    "for fips in FIPS:\n",
    "    # County endpoint\n",
    "    c_end = \"filter=countyCode%20eq%20%27\" + fips + \"%27\"\n",
    "    \n",
    "    # First, get the total number of records\n",
    "    records = requests.get(check + c_end)\n",
    "    n_rec = pd.json_normalize(records.json())['metadata.count'][0]\n",
    "    \n",
    "    # Get iterations needed (1,000 record limit)\n",
    "    iterations = math.ceil(n_rec / CHUNK_FEMA)\n",
    "    \n",
    "    # Now, download 1,000 records at a time and store in list\n",
    "    # Loop through required iterations and keep appending claimicy \n",
    "    # data from the GET request to the claim_df_list\n",
    "    for i in range(iterations):\n",
    "        skip_str = \"&$skip=\" + str(i*CHUNK_FEMA)\n",
    "    \n",
    "        # GET Request\n",
    "        claim_get = requests.get(url + c_end + skip_str)\n",
    "\n",
    "        # Temp data frame\n",
    "        temp = pd.json_normalize(claim_get.json()['FimaNfipClaims'])\n",
    "\n",
    "        # Add to list\n",
    "        claim_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nfip_claim = pd.concat(claim_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nfip_claim.to_parquet(join(EXP_DIR_R, 'nfip_claims.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40910ef",
   "metadata": {},
   "source": [
    "# Vulnerability Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429745a",
   "metadata": {},
   "source": [
    "## Physical Vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18d2ea",
   "metadata": {},
   "source": [
    "## Social Vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3d145",
   "metadata": {},
   "source": [
    "# Administrative Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1951e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ef603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icom_risk",
   "language": "python",
   "name": "icom_risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
