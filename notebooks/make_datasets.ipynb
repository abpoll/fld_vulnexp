{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3033b7e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T22:01:29.223801Z",
     "start_time": "2023-08-24T22:01:29.191599Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7682daa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T22:01:32.837150Z",
     "start_time": "2023-08-24T22:01:29.563941Z"
    }
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import zipfile_deflate64\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import glob\n",
    "from os.path import join\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio \n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff387690",
   "metadata": {},
   "source": [
    "# Configure files and other info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12ca48fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T22:13:32.230506Z",
     "start_time": "2023-08-24T22:13:31.105250Z"
    }
   },
   "outputs": [],
   "source": [
    "# It could make sense to have a lib/ style directory\n",
    "# like PLACES has for common functionality\n",
    "# and this code block would be useful there for getting\n",
    "# a fr() path\n",
    "\n",
    "# Filepath directories\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "# Which is one directory above notebooks/\n",
    "ABS_DIR = os.path.abspath(Path(os.getcwd()).parents[0])\n",
    "# Get raw data directory\n",
    "FR = join(ABS_DIR, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "FI = join(ABS_DIR, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "FP = join(ABS_DIR, 'data', 'processed')\n",
    "\n",
    "# Directories for raw exposure, vulnerability (vuln) and \n",
    "# administrative reference files\n",
    "#  all exist so just need references\n",
    "EXP_DIR_R = join(FR, 'exposure')\n",
    "VULN_DIR_R = join(FR, 'vuln')\n",
    "REF_DIR_R = join(FR, 'ref')\n",
    "# Haz is for FEMA NFHL and depth grids\n",
    "HAZ_DIR_R = join(FR, 'haz')\n",
    "\n",
    "# Directories for interim exposure, vulnerability (vuln) and \n",
    "# hazard\n",
    "EXP_DIR_I = join(FI, 'exposure')\n",
    "VULN_DIR_I = join(FI, 'vuln')\n",
    "HAZ_DIR_I = join(FI, 'haz')\n",
    "\n",
    "# Ensure they exist\n",
    "Path(EXP_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(VULN_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(HAZ_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reference fips\n",
    "FIPS = '42101'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6f226",
   "metadata": {},
   "source": [
    "# Unzip and move files to interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932542b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T21:32:16.763360Z",
     "start_time": "2023-08-22T21:31:26.237921Z"
    }
   },
   "outputs": [],
   "source": [
    "# For each .zip directory in fr\n",
    "# Create needed subdirectories in interim/\n",
    "# Unzip in the appropriate interim/ subdirectory\n",
    "\n",
    "for path in Path(FR).rglob('*.zip'):\n",
    "    # Avoid hidden files and files in directories\n",
    "    if path.name[0] != '.':\n",
    "        # Get root for the directory this .zip file is in\n",
    "        zip_root = path.relative_to(FR).parents[0]\n",
    "\n",
    "        # Get path to interim/zip_root\n",
    "        zip_to_path = join(FI, zip_root)\n",
    "\n",
    "        # Make directory, including parents\n",
    "        # No need to check if directory exists bc\n",
    "        # it is only created when this script is run\n",
    "        Path(zip_to_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Unzip to zip_to_path\n",
    "        with ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(zip_to_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67db191",
   "metadata": {},
   "source": [
    "# Obtain base data for structure inventories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8988746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T17:23:21.647614Z",
     "start_time": "2023-08-23T17:23:20.602752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parcels are the core exposure element \n",
    "# of building-based flood risk assessments\n",
    "# This can be represented by appraisal or NSI data\n",
    "\n",
    "# First, we want to clean and filter the parcel dataset for\n",
    "# the information we need for our flood risk assessment\n",
    "# Then, we want to link parcels with all of the other information\n",
    "# that we will use in our flood risk assessment\n",
    "# This includes attribute links and spatial links\n",
    "# for things like footprints, flood hazard, administrative references\n",
    "\n",
    "# We want to link parcels to other spatial features in the\n",
    "# CRS of those spatial features. This will limit \n",
    "# expensive spatial processing for tasks like reprojection. For example,\n",
    "# if we were to reproject every flood hazard depth grid to the\n",
    "# parcel CRS, we would need to perform many redundant reprojection tasks.\n",
    "# Instead, we could reproject the parcels one time and overlay this with\n",
    "# all the depth grids. A downside of this approach is that if we want to\n",
    "# plot depth grids and parcels, we would likely want to do this in \n",
    "# the parcel CRS. Since plotting is an occassional task, but linking\n",
    "# parcels & depths is a ubiquitous one, I think it's better to do the\n",
    "# parcel reprojection for spatial merges. There are also layers that will\n",
    "# share the CRS of the parcels since WGS84 is very common for parcel\n",
    "# datasets, reference files like block groups, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cbb1a",
   "metadata": {},
   "source": [
    "## Process NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9722cc9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T17:44:49.815706Z",
     "start_time": "2023-08-23T17:44:48.759721Z"
    }
   },
   "outputs": [],
   "source": [
    "# The NSI comes with all the data necessary for performing a standard \n",
    "# flood risk assessment. It is still useful to process the raw data.\n",
    "# Here, we subset to residential properties with 1 to 2 stories\n",
    "# and save as a geodataframe. These are the types of residences we have\n",
    "# multiple depth-damage functions for and a literature base to draw \n",
    "# from to introduce uncertainty in these loss estimates\n",
    "\n",
    "# We are going to use the following occupancy types \n",
    "# RES1 - single family residences\n",
    "# Res2 - manufactured home\n",
    "# RES 3 - multifamily residences\n",
    "# Only if they have <= 2 stories (we lack info for specing \n",
    "# the kinds of DDFs we'd like to -- w/ uncertainty -- for 3 story houses)\n",
    "# RES2 \n",
    "# While DDFs are not specific to manufactured homes, as long\n",
    "# as we know basement type, foundation height, and\n",
    "# stories, we can use the requisite DDF archetype\n",
    "\n",
    "# Future versions of this framework will not subset to the residential\n",
    "# properties as done here, but since we don't have the \n",
    "# requisite depth-damage function information for other properties,\n",
    "# it would be a waste of storage space and processing time to \n",
    "# save/process a larger NSI .gpkg file at this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56792e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T18:08:49.450731Z",
     "start_time": "2023-08-23T18:08:47.736047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read raw NSI data\n",
    "nsi_filep = join(EXP_DIR_R, 'nsi.pqt')\n",
    "# Read and reset index\n",
    "nsi_full = pd.read_parquet(nsi_filep).reset_index(drop=True)\n",
    "\n",
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_full['properties.x'],\n",
    "                             nsi_full['properties.y'])\n",
    "# The NSI CRS is EPSG 4326\n",
    "nsi_gdf = gpd.GeoDataFrame(nsi_full, geometry=geometry,\n",
    "                           crs=\"EPSG:4326\")\n",
    "\n",
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f7c4358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T18:16:26.747506Z",
     "start_time": "2023-08-23T18:16:26.228788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to residential properties and update\n",
    "# occtype category for easier use in loss estimation steps\n",
    "\n",
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['st_damcat'] == 'RES']\n",
    "\n",
    "\n",
    "# Add #story and wb (with basement) or nb (no basement) to RES3 homes\n",
    "# Store NB or WB indexed to RES3 homes based on B,C and N found_type\n",
    "# Get num_story + 'S' \n",
    "# Merge these and then add to occtype for RES3 homes\n",
    "\n",
    "# Start with index of res3 homes\n",
    "res3_ind = nsi_res['occtype'].str[:4] == 'RES3'\n",
    "# Get subsetted df\n",
    "res3 = nsi_res.loc[res3_ind]\n",
    "\n",
    "# For this subset\n",
    "# If found_type == B, then WB\n",
    "# Else then NB\n",
    "res3b = np.where(res3['found_type'] == 'B',\n",
    "                 'WB',\n",
    "                 'NB')\n",
    "# For this subset\n",
    "# Get num_story + 'S'\n",
    "res3s = res3['num_story'].astype(str) + 'S'\n",
    "\n",
    "# Adjust occtype column for these homes in nsi_res\n",
    "nsi_res.loc[res3_ind, 'occtype'] = res3['occtype'] + '-' + res3s + res3b\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res['num_story'] > 2\n",
    "# Final residential dataframe\n",
    "res_f = nsi_res.loc[~res1_3s_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "815c4f09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T18:31:03.442356Z",
     "start_time": "2023-08-23T18:29:25.950916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to relevant columns\n",
    "cols = ['fd_id', 'occtype', 'found_type', 'cbfips',\n",
    "        'ftprntsrc', 'found_ht', 'val_struct',\n",
    "        'val_cont', 'source', 'firmzone', 'ground_elv_m',\n",
    "        'geometry']\n",
    "\n",
    "res_out = res_f.loc[:,cols]\n",
    "\n",
    "# Write out to interim/exposure/\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, 'nsi_res.gpkg')\n",
    "nsi_res.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ad3ca",
   "metadata": {},
   "source": [
    "## Process appraiser data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "495d38f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T22:55:00.511974Z",
     "start_time": "2023-08-24T22:54:00.456629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_974319/1962128361.py:5: DtypeWarning: Columns (12,21,25,30,54,67,69,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pc_opa = pd.read_csv(join(EXP_DIR_R, 'pc_opa.csv'))\n"
     ]
    }
   ],
   "source": [
    "# Load philadelphia water department parcels\n",
    "pc_pwd = gpd.read_file(join(EXP_DIR_R, 'pc_pwd.gpkg'))\n",
    "\n",
    "# Load philadelphia office of property assessment parcels\n",
    "pc_opa = pd.read_csv(join(EXP_DIR_R, 'pc_opa.csv'))\n",
    "\n",
    "# Load building footprint data\n",
    "bld_fp = gpd.read_file(join(EXP_DIR_R, 'bld_fp.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "efaaec69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T22:55:01.644682Z",
     "start_time": "2023-08-24T22:55:00.529169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove \"properties.\" from the pc_pwd column names\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in pc_pwd.columns]\n",
    "pc_pwd.columns = col_updates\n",
    "\n",
    "# Retain relevant columns\n",
    "col_keep = ['ADDRESS', 'PARCELID', 'ADDRESS',\n",
    "            'PIN', 'BRT_ID', 'geometry']\n",
    "pc_pwd = pc_pwd.loc[:, col_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2793bca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T22:55:01.752824Z",
     "start_time": "2023-08-24T22:55:01.646668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove \"properties.\" from the bld_fp column names\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in bld_fp.columns]\n",
    "bld_fp.columns = col_updates\n",
    "\n",
    "# Retain relevant columns\n",
    "col_keep = ['BIN', 'ADDRESS', 'BASE_ELEVATION',\n",
    "            'PARCEL_ID_NUM', 'Shape__Area', 'geometry']\n",
    "bld_fp = bld_fp.loc[:, col_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43898f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T22:55:01.773098Z",
     "start_time": "2023-08-24T22:55:01.755155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter pc_opa and retain relevant columns\n",
    "# Reference https://metadata.phila.gov/#home/datasetdetails/\n",
    "# 5543865f20583086178c4ee5/representationdetails/55d624fdad35c7e854cb21a4/\n",
    "\n",
    "# assessment_date: important for understanding the potential\n",
    "# for renovations/improvements/degradation\n",
    "\n",
    "# basements: detailed fields can be aggregated for whether a building\n",
    "# has a basement or not. whether it is finished or not does not matter\n",
    "# for flood risk estimation other than the effect it may have on \n",
    "# structure value - this is an \"observed\" field with a different column\n",
    "\n",
    "# category_code: identify residential properties. subset to these\n",
    "# 1 - single family, 2 - multi family,\n",
    "# 3 - mixed used, 14 - apartments > 4 units\n",
    "\n",
    "# market_value: certified market value (includes land and buildings)\n",
    "\n",
    "# market_value_date: important for getting market value in \n",
    "# correct real value, but it is fully null. so we will use\n",
    "# assessment_date as a proxy for this. \n",
    "\n",
    "# number_stories: # stories above ground level\n",
    "\n",
    "# parcel_number: matches brt_id in PWD parcels\n",
    "\n",
    "# year_built: useful for determining which building codes\n",
    "# construction was subject to\n",
    "\n",
    "# year_built_estimate: indicates if the year_built column is an estimate\n",
    "\n",
    "# pin: use to link to PIN in PWD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a8714a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T23:02:45.360188Z",
     "start_time": "2023-08-24T23:02:43.225425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number OPA Properties: 582341\n",
      "Number OPA Properties in category codes 1, 2, 3, and 14: 521059\n",
      "Number OPA Properties with 1 or 2 Stories: 437855\n",
      "Number OPA Properties with non-zero or non-null market values: 437844\n"
     ]
    }
   ],
   "source": [
    "# How many observations do we start with\n",
    "print('Number OPA Properties: ' + str(len(pc_opa)))\n",
    "\n",
    "# Filter pc_opa to category codes 1, 2, 3, and 14\n",
    "cat_codes = [1, 2, 3, 14]\n",
    "pc_opa_f = pc_opa.loc[pc_opa['category_code'].isin(cat_codes)]\n",
    "\n",
    "print('Retained Properties in category codes 1, 2, 3, and 14: ' \n",
    "      + str(len(pc_opa_f)))\n",
    "\n",
    "# Filter to number stories 1 or 2\n",
    "# FLAG: should we take the assessor data at face value?\n",
    "pc_opa_f = pc_opa_f.loc[(pc_opa_f['number_stories'] == 1) |\n",
    "                        (pc_opa_f['number_stories'] == 2)]\n",
    "\n",
    "print('Retained Properties with 1 or 2 Stories: ' \n",
    "      + str(len(pc_opa_f)))\n",
    "\n",
    "# Keep properties with non-zero and non-null market values\n",
    "pc_opa_f = pc_opa_f.loc[(pc_opa_f['market_value'].notnull())\n",
    "                        & (pc_opa_f['market_value'] != 0)]\n",
    "\n",
    "# Retain relevant columns\n",
    "keep_cols = ['assessment_date', 'category_code',\n",
    "             'market_value', 'number_stories',\n",
    "             'basements',\n",
    "             'year_built', 'year_built_estimate',\n",
    "             'parcel_number', 'pin',]\n",
    "pc_opa_f = pc_opa_f.loc[:, keep_cols]\n",
    "\n",
    "# How many observations do we filter to\n",
    "print('Retained Properties with non-zero or non-null market values: ' \n",
    "      + str(len(pc_opa_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "696a6cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T23:10:20.012387Z",
     "start_time": "2023-08-24T23:10:00.890983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retain properties with an assessment date \n",
    "pc_opa_f = pc_opa_f[pc_opa_f['assessment_date'].notnull()]\n",
    "\n",
    "# Get year of assessment date\n",
    "# Have to split the string to use pd.to_datetime successfully\n",
    "dates = pd.to_datetime(pc_opa_f['assessment_date'].str.split(' ').str[0],\n",
    "                       format=\"%Y-%m-%d\")\n",
    "years = dates.dt.year\n",
    "\n",
    "# Use the column name \"Year\" to match with the hpi data\n",
    "# for better syntax on merge\n",
    "pc_opa_f = pc_opa_f.assign(Year = years)\n",
    "\n",
    "# Deflate market values in terms of 2022 value\n",
    "# For assessments in 2022, no adjustment needed\n",
    "\n",
    "# Read hpi deflator data\n",
    "hpi_path = join(EXP_DIR_R, 'hpi_county.xlsx')\n",
    "# Manual inspection of file provides the values \n",
    "# for skiprows, HPI as float, FIPS as str\n",
    "hpi = pd.read_excel(hpi_path, skiprows=6, dtype={'FIPS code': 'str'})\n",
    "\n",
    "# Subset to our county\n",
    "hpi_fips = hpi.loc[hpi['FIPS code'] == FIPS]\n",
    "\n",
    "# I can't do dtype 'HPI': 'float' but we need to convert the column\n",
    "# and the below works fine\n",
    "hpi_fips = hpi_fips.assign(hpi=hpi_fips['HPI'].astype(float))\n",
    "\n",
    "# Get HPI with 2022 base\n",
    "# TODO: Define in cfg file\n",
    "YR_BASE = 2022\n",
    "\n",
    "# Do this by dividing HPI values by the HPI value \n",
    "# for the row with Year = 2022\n",
    "# Round to hundredth place\n",
    "# TODO: better column name than hpi_2022 (hpi_base?)\n",
    "base_hpi = hpi_fips.loc[hpi_fips['Year'] == YR_BASE, 'hpi'].values[0]\n",
    "rounded_hpi = round(hpi_fips.loc[:, 'hpi']/base_hpi, 2)\n",
    "hpi_fips = hpi_fips.assign(hpi_2022=round(hpi_fips['hpi']/base_hpi, 2))\n",
    "\n",
    "# Merge the ratio with the parcel dataframe\n",
    "# TODO: it could make sense to add FIPS as a merge\n",
    "# when expanding to more locations\n",
    "# I like changing df names after merges because\n",
    "# if you need to rerun a code block it will work without\n",
    "# column name or index issues\n",
    "pc_opa_f2 = pc_opa_f.merge(hpi_fips[['Year', 'hpi_2022']],\n",
    "                           on='Year',\n",
    "                           how='left')\n",
    "\n",
    "# Making the assumption that assessments in 2023 have \n",
    "# similar value to those from 2022, otherwise we get null entries\n",
    "pc_opa_f2.loc[pc_opa_f2['Year'] == 2023, 'hpi_2022'] = 1\n",
    "\n",
    "# Scale the market value by 1/ratio \n",
    "pc_opa_f2 = pc_opa_f2.assign(bld_mv_2022=round(.8*pc_opa_f2['market_value']\n",
    "                             /pc_opa_f2['hpi_2022']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "73879c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T00:18:56.809873Z",
     "start_time": "2023-08-25T00:18:56.714495Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map basements column to basement types\n",
    "# 0. None – Indicates no basement.\n",
    "# A. Full Finished – Occupies the entire area under the first floor.\n",
    "# B. Full Semi-Finished – Could have some finish to include a floor covering,\n",
    "# and ceiling. It looks more like a living area rather than a basement.\n",
    "# C. Full Unfinished – Is a typical basement with unfinished concrete floor,\n",
    "# either rubble stone or cement over stone or concrete walls and would have\n",
    "# exposed wood joist ceilings.\n",
    "# D. Full – Unknown Finish\n",
    "# E. Partial Finished – Occupies a portion under the first floor. Be careful of\n",
    "# areas under sheds and porches. If there is a garage at basement level then it is a\n",
    "# partial basement.\n",
    "# F. Partial Semi-Finished – One or more finished areas.\n",
    "# G. Partial Unfinished\n",
    "# H. Partial - Unknown Finish\n",
    "# I. Unknown Size - Finished\n",
    "# J. Unknown Size - Unfinished\n",
    "\n",
    "# From my property.phila.gov, google maps, opa data groundtruth checks,\n",
    "# each example of null in opa data matches no basement in property.phila.gov\n",
    "# and seems visually plausible. So, fill na with 0 which corresponds to\n",
    "# no basement\n",
    "# The categories w/ letters should be marked as basements\n",
    "# There is no way to know which of the no basement homes are crawl\n",
    "# space homes versus slab or other type of foundation\n",
    "# Further, it's unclear if some crawl space homes may be considered\n",
    "# partial basements. It doesn't appear like this is the case, but\n",
    "# it's uncertain. The way I'm going to do this, na cells become no basement\n",
    "# and we will split no basement into slab & crawl space foundation types\n",
    "# based on NFIP Poliices foundation type proportions\n",
    "# I have no information on pilings/piers and other raised foundations\n",
    "# so will assume they are not in Philadelphia. This could be wrong but \n",
    "# I don't have other information about this\n",
    "\n",
    "# There are also categories 1, 2, 3, and 4 which have no description\n",
    "# in the metadata. So will treat these as no basement\n",
    "# Searching on property.phila.gov, there is nothing said about\n",
    "# the basement type for the properties I've checked\n",
    "\n",
    "# basement or not columns\n",
    "pc_opa_f2['b_type'] = 'NB'\n",
    "# if basements not 0 or null, 'WB'\n",
    "pc_opa_f2.loc[(pc_opa_f2['basements'].notnull()) &\n",
    "              ~(pc_opa_f2['basements'].isin(['0', '1', '2', '3', '4'])),\n",
    "              'b_type'] = 'WB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41365410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any redundant columns (i.e. don't need Year anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge pwd with bld_fp\n",
    "# Keep the bld_fp polygons\n",
    "\n",
    "# Merge resultant gdf by PIN/pin with opa\n",
    "# Check how results compare to merging on brt and parcel_number\n",
    "\n",
    "# Only keep properties with a match. Remainder are ambiguous\n",
    "# Instructed by OPA that PWD and condo matching will be mostly\n",
    "# unsuccessful with these methods. Condos may need more attention later\n",
    "# Discrepancies in loss estimates may be due to ambiguous processing \n",
    "# decisions about which buildings to include and how to estimate losses\n",
    "# for them. An important point may be that reporting absolute losses\n",
    "# is so sensitive to an uncertain extensive margin that it should not\n",
    "# be done without explicitly reporting how many properties are ambiguous\n",
    "# property types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b512143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results of merge and determine which building_code_description\n",
    "# parcels to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31644d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out building footprint centroids for parcel ids\n",
    "# Some parcels have multiple buildings on them (these are tax parcels)\n",
    "# and we want to have unique ids for these\n",
    "\n",
    "# Write out the unique ids as .pqt with relevant columns for loss estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match pc_pwd and pc_opa on PIN\n",
    "# The PARCELID column in the resulting dataset\n",
    "# can be used to link to PARCEL_ID_ in bld_fp\n",
    "# This avoids relying on potentially inaccurate spatial joins\n",
    "# since we only have point data for opa parcels and \n",
    "# footprint data for buildings\n",
    "# TODO: The more general processing step is for\n",
    "# parcel polygon and building footprint joins. Not all \n",
    "# future sources of appraiser data will provide the links\n",
    "# like the Philadelphia data from PWD. For best performance in our\n",
    "# case-study, it makes sense to use the pre-specified links\n",
    "# of PWD parcels to building footprints. It would be valuable\n",
    "# to benchmark different spatial joining approaches of \n",
    "# PWD parcels and footprints to guide best practices for\n",
    "# these kinds of joins for data that doesn't have pre specified links\n",
    "# This will be the most common use case since building footprint data\n",
    "# is rarely made available from the municipality and often comes\n",
    "# from an external source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d599c",
   "metadata": {},
   "source": [
    "# Link properties to spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd96b24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T17:28:51.784301Z",
     "start_time": "2023-08-23T17:28:50.808926Z"
    }
   },
   "outputs": [],
   "source": [
    "# NSI and appraiser datasets have building footprint spatial references\n",
    "# We will use these, and their unique ids, to link the structure\n",
    "# inventories to spatial data like flood zones, depth grids, \n",
    "# reference data, and tabular data (which often can be made through shared\n",
    "# references like zip codes or census tracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c9c04",
   "metadata": {},
   "source": [
    "## Hazard data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44500fc",
   "metadata": {},
   "source": [
    "## Reference data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b900919a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T21:51:39.014509Z",
     "start_time": "2023-08-22T21:51:38.327848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates', 'index_right']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7061423c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T21:54:55.507123Z",
     "start_time": "2023-08-22T21:52:50.150285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the NSI data to interim\n",
    "nsi_gdf.to_file(join(EXP_DIR_I, 'nsi.gpkg'), driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551244d",
   "metadata": {},
   "source": [
    "## Depth Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a92d05",
   "metadata": {},
   "source": [
    "# Prepare depth-damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e00469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icom_risk",
   "language": "python",
   "name": "icom_risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "322.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
